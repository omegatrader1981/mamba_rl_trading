# Base configuration file
defaults:
  - experiment: smoke_test
  - instrument: mnq
  - _self_

experiment:
  name: "DefaultExperiment"
  description: "Base experiment fallback"

data:
  train_start: "2020-01-01"
  train_end: "2021-12-31"
  val_start: "2022-01-01"
  val_end: "2022-12-31"
  test_start: "2023-01-01"
  test_end: "2024-12-31"
  
  cleaning:
    min_allowable_price: 0.01
    outlier_rolling_window: 21
    outlier_median_dev_threshold: 0.20
  
  regime_definitions:
    train_High_Vol_Crash:
      - ["2020-02-15", "2020-04-30"]
    train_Bull_Market:
      - ["2020-06-01", "2020-09-30"]
    train_Bear_Market:
      - ["2022-04-01", "2022-07-31"]
    train_Sideways_Chop:
      - ["2023-07-01", "2023-10-31"]
    train_Low_Vol_Grind:
      - ["2021-10-01", "2021-12-31"]
    
    validation_set:
      - ["2022-01-01", "2022-03-31"]
    
    test_set:
      - ["2024-01-01", "2024-03-31"]
    
    val_2022_Q1:
      - ["2022-01-01", "2022-03-31"]
    val_2022_Q4:
      - ["2022-10-01", "2022-12-31"]
    test_2023_full:
      - ["2023-01-01", "2023-12-31"]
    test_2024_Q1:
      - ["2024-01-01", "2024-03-31"]
    test_2024_full:
      - ["2024-01-01", "2024-12-31"]

features:
  pct_change_cap: 0.1
  hl_pct_cap: 0.2
  z_score_window: 100
  hmm_n_components: 3
  hmm_min_samples: 1000

env:
  lookback_window: 20
  features:
    - "returns"
    - "log_returns"
    - "volatility"
    - "hurst_exponent"
    - "session_vwap_distance"
    - "risk_adjusted_momentum"
    - "hour_sin"
    - "hour_cos"
    - "day_of_week_sin"
    - "day_of_week_cos"
    - "hmm_regime"

model:
  policy_type: "MlpPolicy"
  mlp_activation_fn_name: "relu"
  mamba_activation_fn_name: "silu"
  mamba_d_conv_default: 4
  mamba_expand_default: 2
  policy_kwargs:
    net_arch:
      pi: [256, 256]
      vf: [256, 256]
    activation_fn: "tanh"

environment:
  initial_balance: 100000
  max_episode_steps: 10000
  tick_size: 0.25
  max_slippage_points: 0.5
  add_bid_ask_spread: false
  spread_ticks_min: 1
  spread_ticks_max: 2
  max_consecutive_holds_limit: 900
  excessive_hold_penalty: 0.5
  reward_scheme: "sac_shaped"
  action_threshold: 0.5
  activity_bonus_scale: 0.0
  hold_penalty_scale: 0.0
  win_bonus_scale: 0.0
  loss_penalty_scale: 0.0
  unrealized_loss_penalty_scale: 0.0

training:
  total_timesteps: 1000000
  n_eval_episodes: 10
  eval_freq: 50000
  save_freq: 100000
  verbose: 1
  device: auto

evaluation:
  deterministic_actions: true

optimization:
  enabled: true
  n_trials: 50
  timeout: 7200
  n_jobs: 1

saving:
  output_dir: output_mamba_mnq_${experiment.name}
  scaler_filename: mnq_scaler_${experiment.name}.joblib
  optuna_study_name: mnq_Opt_${experiment.name}
  optuna_db_name: optuna_study_mnq_${experiment.name}.db
  best_params_filename: mnq_best_params_${experiment.name}.joblib
  final_model_filename: mnq_final_model_${experiment.name}.zip
  vec_normalize_filename: mnq_vecnormalize_final_${experiment.name}.pkl
  trades_filename: test_trades_${experiment.name}.csv
  equity_filename: test_equity_${experiment.name}.csv
  report_filename: mnq_report_${experiment.name}.html
  log_filename: mamba_rl_trading_${experiment.name}.log
  checkpoint_dir_name: checkpoints_${experiment.name}
  equity_curve_plot: "equity_curve.png"
  positions_plot: "positions.png"
