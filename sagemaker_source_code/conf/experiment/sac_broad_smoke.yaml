# @package _global_

defaults:
  - override /optimization: enabled

experiment:
  name: SAC_Broad_Smoke
  agent_type: sac

training:
  total_timesteps: 10000

optimization:
  enabled: true
  n_trials: 2
  trial_timesteps: 5000
  metric: 'sharpe'
  direction: 'maximize'
  pruning:
    enabled: false

  lr_min: 1e-4
  lr_max: 3e-4
  batch_size_choices: [256]
  buffer_size_choices: [50000]
  gamma_min: 0.99
  gamma_max: 0.99
  tau_min: 0.01
  tau_max: 0.01
  train_freq_choices: [16]
  learning_starts_choices: [1000]

  activity_bonus_scale_min: 0.0
  activity_bonus_scale_max: 0.01
  hold_penalty_scale_min: 0.0
  hold_penalty_scale_max: 0.01
  win_bonus_scale_min: 0.1
  win_bonus_scale_max: 0.5
  loss_penalty_scale_min: 0.5
  loss_penalty_scale_max: 1.0
  unrealized_loss_penalty_scale_min: 0.0
  unrealized_loss_penalty_scale_max: 0.05

  weight_decay_min: 0.0
  weight_decay_max: 1e-5
  mamba_layers_min: 2
  mamba_layers_max: 2
  mamba_d_model_choices: [64]
  mamba_d_state_choices: [16]
  features_dim_choices: [128]
  dropout_min: 0.0
  dropout_max: 0.1

environment:
  reward_scheme: 'sac_shaped'


  regime_definitions:
    train_trending:
      - ["2024-01-01", "2024-01-07"]  # 1 week of data

saving:
  output_dir: output_smoke
  scaler_filename: mnq_scaler_smoke.joblib
  optuna_study_name: mnq_Opt_Smoke
  optuna_db_name: optuna_study_mnq_smoke.db
  best_params_filename: mnq_best_params_smoke.joblib
  final_model_filename: mnq_final_model_smoke.zip
  vec_normalize_filename: mnq_vecnormalize_final_smoke.pkl
  trades_filename: test_trades_smoke.csv
  equity_filename: test_equity_smoke.csv
  report_filename: mnq_report_smoke.html
  log_filename: mamba_rl_trading_smoke.log
  checkpoint_dir_name: 'checkpoints_smoke'
